# Neural-Machine-Translation-NMT-using-FairSeq-and-OpenNMT
**Natural Language Processing course - CA5**  

## Abstract
This is a collaborative project on Neural Machine Translation (NMT) by employing two renowned tools (FairSeq, OpenNMT). 
## Preprocessing
The first step was processing, which encompasses utilizing BPE for tokenization (we used the subword-nmt library); moreover, converting all the letters into lowercase. 
## FairSeq
Considering FairSeq, we had to specify some hyperparameters for the model, such as the loss function, optimizer, batch size, and so forth.
Here is the sample output when training the model with the FairSeq tool:

![fig1](https://github.com/vassef/Neural-Machine-Translation-using-FairSeq-and-OpenNMT/blob/9243c47626d67c1c40de39f22244d81121d04ff4/Figures/FairSeq.png)

## OpenNMT
The same procedure was applied to OpenNMT, and the sample output is as follows:

![fig2](https://github.com/vassef/Neural-Machine-Translation-using-FairSeq-and-OpenNMT/blob/240eeb6161f7f11d62e9d078ad13164bd449a8b7/Figures/OpenNMT.png)

